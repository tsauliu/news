'''
在远程服务器上host一个文件服务器,可以access folder
TODO: 1. 合并wewe-rss.db 和我新抓的db，生成新db，只看过去7天发布的文章
'''
# %%
"""
在远程服务器上host一个文件服务器,可以access folder
TODO: 1. 合并wewe-rss.db 和我新抓的db，生成新db，只看过去7天发布的文章

This script now supports a simple source switch via parameters.ARTICLE_SOURCE:
- 'remote_db': use remote/local wechat SQLite DBs
- 'rss': use rss_articles.csv generated by 0_RSS.py

No merging between RSS and remote_db in a single run.
"""
# extract news from sqlite or rss csv depending on source
import sqlite3
import pandas as pd
import os
from parameters import friday_date, download_file, ARTICLE_SOURCE
from apikey import news_start

import requests
import shutil

folder_path = f'data/1_urls/'
os.makedirs(folder_path, exist_ok=True)

if ARTICLE_SOURCE == 'remote_db':
    # read the remote database file
    local_db_path = 'data/wechat_articles.db'

    try:
        remote_db_url = "http://118.193.44.18:8000/data/wechat_articles.db"
        os.makedirs('data', exist_ok=True)
        download_file(remote_db_url, local_db_path)
    except Exception as e:
        print(f"Error downloading remote database file: {e}")

    conn_wechat = sqlite3.connect(local_db_path)
    wechat_articles = pd.read_sql_query("SELECT * FROM articles", conn_wechat)
    conn_wechat.close()
    wechat_articles['pub_time'] = pd.to_datetime(
        wechat_articles['pub_time'], format='%Y年%m月%d日 %H:%M', errors='coerce'
    ).dt.strftime('%Y-%m-%d %H:%M:%S')
    wechat_articles['source'] = 'wechat'
    wechat_articles.rename(
        columns={'pub_time': 'publish_time', 'article_title': 'title', 'channel_scraped': 'mp_name'},
        inplace=True,
    )
    wechat_articles = wechat_articles[
        ['mp_name', 'title', 'url', 'publish_time', 'source']
    ].sort_values(by='publish_time', ascending=False)

    # read the local database file (wewe-rss)
    conn = sqlite3.connect('data/wewe-rss.db')
    articles = pd.read_sql_query("SELECT * FROM articles", conn)
    articles['publish_time'] = pd.to_datetime(articles['publish_time'], unit='s', utc=True).dt.tz_convert('Asia/Shanghai').dt.strftime('%Y-%m-%d %H:%M:%S')
    articles['created_at'] = pd.to_datetime(articles['created_at'], unit='ms', utc=True).dt.tz_convert('Asia/Shanghai').dt.strftime('%Y-%m-%d %H:%M:%S')
    articles['updated_at'] = pd.to_datetime(articles['updated_at'], unit='ms', utc=True).dt.tz_convert('Asia/Shanghai').dt.strftime('%Y-%m-%d %H:%M:%S')
    articles['url'] = 'https://mp.weixin.qq.com/s/' + articles['id']
    feeds = pd.read_sql_query("SELECT * FROM feeds", conn)
    article_clean = articles[['mp_id', 'title', 'publish_time', 'url']].merge(
        feeds.rename(columns={'id': 'mp_id'})[['mp_id', 'mp_name']], on='mp_id', how='left'
    ).drop(columns=['mp_id'])
    article_clean.sort_values(by='publish_time', ascending=False, inplace=True)
    article_clean['source'] = 'wewerss'

    # Merge both wechat sources, dedupe by url
    article_clean = pd.concat([article_clean, wechat_articles]).drop_duplicates(subset=['url'])

    # save the article_clean to csv (recent and full)
    article_recent = article_clean[
        pd.to_datetime(article_clean['publish_time']) >= (pd.to_datetime(friday_date) - pd.Timedelta(days=news_start))
    ].sort_values(by='publish_time', ascending=False)

    article_recent[['publish_time', 'mp_name', 'title', 'url', 'source']].to_csv(
        os.path.join(folder_path, f'{friday_date}_article_urls.csv'), index=False
    )
    article_clean.sort_values(by='publish_time', ascending=False)[
        ['publish_time', 'mp_name', 'title', 'url', 'source']
    ].to_excel(os.path.join(folder_path, f'article_urls.xlsx'), index=False)

    print(f"{friday_date}_article_urls.csv saved (remote_db mode)")
    print(f"{len(article_clean)} articles saved")

elif ARTICLE_SOURCE == 'rss':
    # Build URLs from rss_articles.csv produced by 0_RSS.py
    rss_path = 'data/rss_articles.csv'
    if not os.path.exists(rss_path):
        raise FileNotFoundError("data/rss_articles.csv not found. Run 0_RSS.py first or set ARTICLE_SOURCE=remote_db.")

    rss_articles = pd.read_csv(rss_path)
    rss_articles['source'] = 'rss'
    rss_articles['publish_time'] = pd.to_datetime(rss_articles['published'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')
    rss_articles['url'] = rss_articles['link']
    rss_articles['mp_name'] = rss_articles['source_name']
    rss_articles = rss_articles[['mp_name', 'title', 'url', 'publish_time', 'source']].sort_values(by='publish_time', ascending=False)

    recent = rss_articles[
        pd.to_datetime(rss_articles['publish_time']) >= (pd.to_datetime(friday_date) - pd.Timedelta(days=news_start))
    ].sort_values(by='publish_time', ascending=False)

    recent[['publish_time', 'mp_name', 'title', 'url', 'source']].to_csv(
        os.path.join(folder_path, f'{friday_date}_article_urls.csv'), index=False
    )
    rss_articles.to_excel(os.path.join(folder_path, f'article_urls.xlsx'), index=False)

    print(f"{friday_date}_article_urls.csv saved (rss mode)")
    print(f"{len(rss_articles)} RSS articles saved")
else:
    raise ValueError("Unknown ARTICLE_SOURCE. Use 'remote_db' or 'rss'.")
